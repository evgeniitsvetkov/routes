{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `clickstream.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal ../clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 17:01:47,349 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the clickstream data with proper schema and types\n",
    "clickstream_df = se.read.option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv('clickstream.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cast timestamp to integer for processing\n",
    "clickstream_df = clickstream_df.withColumn(\"timestamp\", clickstream_df.timestamp.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify minimum timestamp for sessions containing errors\n",
    "min_timestamp_df = (clickstream_df\n",
    "    .filter(F.col(\"event_type\").contains(\"error\"))  # filter for error events\n",
    "    .groupBy(\"user_id\", \"session_id\")\n",
    "    .agg(F.min(\"timestamp\").alias(\"min_ts\"))  # calculate the minimum timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a window specification to track previous event pages\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the DataFrame to extract valid routes\n",
    "valid_routes_df = (clickstream_df\n",
    "    .join(min_timestamp_df, [\"user_id\", \"session_id\"], \"left\")  # perform a left join\n",
    "    .filter(\n",
    "        (F.col(\"min_ts\").isNull()) | (F.col(\"timestamp\") < F.col(\"min_ts\"))\n",
    "    )  # keep valid sessions\n",
    "    .filter(F.col(\"event_type\") == \"page\")  # keep only page events\n",
    "    .select(\"user_id\", \"session_id\", \"event_page\", \"timestamp\")  # select necessary columns\n",
    "    .withColumn(\"prev_event_page\", F.lag(\"event_page\", 1).over(window_spec))  # get previous page\n",
    "    .filter(\n",
    "        (F.col(\"prev_event_page\").isNull()) | (F.col(\"event_page\") != F.col(\"prev_event_page\"))\n",
    "    )  # remove consecutive duplicates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate pages into routes and count occurrences\n",
    "routes_df = (valid_routes_df\n",
    "    .groupBy(\"user_id\", \"session_id\")\n",
    "    .agg(F.collect_list(\"event_page\").alias(\"pages\"))  # collect pages into a list\n",
    "    .withColumn(\"route\", F.array_join(\"pages\", \"-\"))  # create route string\n",
    "    .groupBy(\"route\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))  # count occurrences of each route\n",
    "    .orderBy(F.desc(\"count\"))  # sort by count in descending order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  676|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|    main-news-rabota|  135|\n",
      "|   main-bonus-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  123|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  114|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The 30 most frequent user routes\n",
    "routes_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the DataFrame to extract valid routes\n",
    "result_df = (clickstream_df\n",
    "    .join(min_timestamp_df, ['user_id', 'session_id'], 'left')  # join with min_timestamp_df\n",
    "    .filter((min_timestamp_df.min_ts.isNull()) | (clickstream_df.timestamp < min_timestamp_df.min_ts))  # filter out invalid sessions\n",
    "    .filter(clickstream_df.event_type == 'page')  # keep only \"page\" events\n",
    "    .select('user_id', 'session_id', 'event_page', 'timestamp')  # select relevant columns\n",
    "    .withColumn('prev_event_page', F.lag('event_page', 1).over(window_spec))  # get the previous event page\n",
    "    .filter((F.col('prev_event_page').isNull()) | (F.col('event_page') != F.col('prev_event_page')))  # remove consecutive duplicates\n",
    "    .groupBy('user_id', 'session_id')  # group by user and session\n",
    "    .agg(F.collect_list('event_page').alias('pages'))  # collect pages in a list\n",
    "    .withColumn('route', F.array_join('pages', '-'))  # join pages into a route string\n",
    "    .groupBy('route')  # group by route\n",
    "    .agg(F.count('*').alias('count'))  # count occurrences of each route\n",
    "    .orderBy(F.desc('count'))  # sort by count in descending order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  676|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|    main-news-rabota|  135|\n",
      "|   main-bonus-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  123|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  114|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The 30 most frequent user routes\n",
    "result_df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstream_rdd = clickstream_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Identify sessions with errors and the corresponding timestamps\n",
    "error_events = (clickstream_rdd\n",
    "    .filter(lambda line: 'error' in line['event_type'])\n",
    "    .map(lambda line: (f\"{line['user_id']}_{line['session_id']}\", int(line['timestamp'])))\n",
    "    .groupByKey()\n",
    "    .map(lambda line:(line[0], min(line[1])))  # find the earliest timestamp of the error\n",
    "    .collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Broadcast the error events map to optimize for large datasets\n",
    "errors_broadcast = sc.broadcast(error_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the clickstream data to filter valid sessions and construct routes\n",
    "import math\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def remove_consecutive_duplicates(obj):\n",
    "    sorted_pages = [i[0] for i in sorted(obj[1], key=lambda o: o[1])]  # sort pages by their timestamp\n",
    "    pages = [key for key, _ in groupby(sorted_pages)]  # remove consecutive duplicates\n",
    "    return (obj[0], pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_routes = (clickstream_rdd\n",
    "    .map(lambda line: (f\"{line['user_id']}_{line['session_id']}\", [line['event_type'], line['event_page'], int(line['timestamp'])]))\n",
    "    .filter(lambda line: line[0] not in errors_broadcast.value or line[1][2] < errors_broadcast.value[line[0]])\n",
    "    .filter(lambda line: line[1][0] == \"page\")  # only keep \"page\" events\n",
    "    .map(lambda line: (line[0], (line[1][1], line[1][2])))  # extract relevant info\n",
    "    .groupByKey()\n",
    "    .map(remove_consecutive_duplicates)  # remove consecutive duplicates\n",
    "    .map(lambda line: (line[0], \"-\".join(line[1])))  # join pages into routes\n",
    "    .map(lambda line: (line[1], 1))  # map routes to counts\n",
    "    .reduceByKey(lambda x, y: x + y)  # sum counts for each route\n",
    "    .sortBy(lambda line: line[1], ascending=False)  # sort routes by count in descending order\n",
    "    .take(30)  # take the top 30 routes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('main', 8184),\n",
       " ('main-archive', 1113),\n",
       " ('main-rabota', 1047),\n",
       " ('main-internet', 897),\n",
       " ('main-bonus', 870),\n",
       " ('main-news', 769),\n",
       " ('main-tariffs', 677),\n",
       " ('main-online', 587),\n",
       " ('main-vklad', 518),\n",
       " ('main-rabota-archive', 170),\n",
       " ('main-archive-rabota', 167),\n",
       " ('main-bonus-archive', 143),\n",
       " ('main-rabota-bonus', 139),\n",
       " ('main-news-rabota', 135),\n",
       " ('main-bonus-rabota', 135),\n",
       " ('main-archive-internet', 132),\n",
       " ('main-rabota-news', 130),\n",
       " ('main-internet-rabota', 129),\n",
       " ('main-archive-news', 126),\n",
       " ('main-rabota-internet', 124),\n",
       " ('main-internet-archive', 123),\n",
       " ('main-archive-bonus', 117),\n",
       " ('main-internet-bonus', 115),\n",
       " ('main-tariffs-internet', 114),\n",
       " ('main-news-archive', 113),\n",
       " ('main-news-internet', 109),\n",
       " ('main-archive-tariffs', 104),\n",
       " ('main-tariffs-archive', 103),\n",
       " ('main-internet-news', 103),\n",
       " ('main-rabota-main', 94)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 30 most frequent user routes\n",
    "filtered_routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_10_routes = {}\n",
    "for k, v in filtered_routes[:10]:\n",
    "    top_10_routes[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main': 8184,\n",
       " 'main-archive': 1113,\n",
       " 'main-rabota': 1047,\n",
       " 'main-internet': 897,\n",
       " 'main-bonus': 870,\n",
       " 'main-news': 769,\n",
       " 'main-tariffs': 677,\n",
       " 'main-online': 587,\n",
       " 'main-vklad': 518,\n",
       " 'main-rabota-archive': 170}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(top_10_routes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "Correct main answer!\n",
      "Correct main-archive answer!\n",
      "Correct main-rabota answer!\n",
      "Correct main-internet answer!\n",
      "Correct main-bonus answer!\n",
      "Correct main-news answer!\n",
      "Correct main-tariffs answer!\n",
      "Correct main-online answer!\n",
      "Correct main-vklad answer!\n",
      "Correct main-rabota-archive answer!\n"
     ]
    }
   ],
   "source": [
    "!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/jenyatsvetkov/w6/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
